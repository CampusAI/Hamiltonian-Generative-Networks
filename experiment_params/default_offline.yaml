# Experiment ID
experiment_id: "default_offline"
model_save_dir: "saved_models"

gpu_id: 0  # Will use this gpu if available

# Define data paths
dataset:
  train_data: "pendulum_data/train"  # Should be of 50k samples
  test_data: "pendulum_data/test"  # Should be of 50k samples
  rollout:
    seq_length: 30
    delta_time: 0.125
    n_channels: 3
    noise_level: 0  # Level of environment noise. 0 means no noise, 1 means max noise.
                    # Maximum values are defined in each environment.

# Define networks architectures
networks:
  variational: True
  potential_learning: False
  dtype : "float"
  encoder:
    hidden_conv_layers: 6
    n_filters: [32, 64, 64, 64, 64, 64, 64]  # first + hidden
    kernel_sizes: [3, 3, 3, 3, 3, 3, 3, 3]  # first + hidden + last
    strides: [1, 1, 1, 1, 1, 1, 1, 1]  # first + hidden + last
    out_channels: 48
  transformer:
    hidden_conv_layers: 1
    n_filters: [64, 64]  # first + hidden
    kernel_sizes: [3, 3, 3]  # first + hidden + last
    strides: [2, 2, 2]  # first + hidden + last
    out_channels: 16  # Channels of q, and p splitted
  hamiltonian:
    hidden_conv_layers: 4
    in_shape: [16, 4, 4]  # Should be coherent with transformer output
    n_filters: [32, 64, 64, 64, 64, 64]  # first + hidden
    kernel_sizes: [3, 3, 3, 3, 3, 3]  # first + hidden + last
    strides: [1, 1, 1, 1, 1, 1]  # first + hidden + last
  decoder:
    n_residual_blocks: 3
    n_filters: [64, 64, 64]
    kernel_sizes: [3, 3, 3, 3]

# Define HGN Integrator
integrator:
  method: "Leapfrog"

# Define optimization
optimization:
  epochs: 5
  batch_size: 16
  # Learning rates
  encoder_lr: 1.5e-4
  transformer_lr: 1.5e-4
  hnn_lr: 1.5e-4
  decoder_lr: 1.5e-4
